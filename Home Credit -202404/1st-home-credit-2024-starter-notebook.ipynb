{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Example Notebook\n\nWelcome to the example notebook for the Home Credit Kaggle competition. The goal of this competition is to determine how likely a customer is going to default on an issued loan. The main difference between the [first](https://www.kaggle.com/c/home-credit-default-risk) and this competition is that now your submission will be scored with a custom metric that will take into account how well the model performs in future. A decline in performance will be penalized. The goal is to create a model that is stable and performs well in the future.\n\nIn this notebook you will see how to:\n* Load the data\n* Join tables with Polars - a DataFrame library implemented in Rust language, designed to be blazingy fast and memory efficient.  \n* Create simple aggregation features\n* Train a LightGBM model\n* Create a submission table\n\n## Load the data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import polars as pl\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score \n\ndataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:38:31.283924Z","iopub.execute_input":"2024-05-14T05:38:31.285927Z","iopub.status.idle":"2024-05-14T05:38:31.436249Z","shell.execute_reply.started":"2024-05-14T05:38:31.285876Z","shell.execute_reply":"2024-05-14T05:38:31.435000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n    # implement here all desired dtypes for tables\n    # the following is just an example\n    for col in df.columns:\n        # last letter of column name will help you determine the type\n        if col[-1] in (\"P\", \"A\"):\n            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n\n    return df\n\ndef convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n    for col in df.columns:  \n        if df[col].dtype.name in ['object', 'string']:\n            df[col] = df[col].astype(\"string\").astype('category')\n            current_categories = df[col].cat.categories\n            new_categories = current_categories.to_list() + [\"Unknown\"]\n            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n            df[col] = df[col].astype(new_dtype)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:22:35.941047Z","iopub.execute_input":"2024-05-14T04:22:35.941628Z","iopub.status.idle":"2024-05-14T04:22:35.950698Z","shell.execute_reply.started":"2024-05-14T04:22:35.941591Z","shell.execute_reply":"2024-05-14T04:22:35.948323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_basetable = pl.read_csv(dataPath + \"csv_files/train/train_base.csv\")\ntrain_static = pl.concat(\n    [\n        pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(set_table_dtypes),\n    ],\n    how=\"vertical_relaxed\",\n)\ntrain_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(set_table_dtypes)\ntrain_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(set_table_dtypes) \ntrain_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/train/train_credit_bureau_b_2.csv\").pipe(set_table_dtypes) ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:22:35.952344Z","iopub.execute_input":"2024-05-14T04:22:35.953561Z","iopub.status.idle":"2024-05-14T04:22:52.174877Z","shell.execute_reply.started":"2024-05-14T04:22:35.953513Z","shell.execute_reply":"2024-05-14T04:22:52.173632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_basetable = pl.read_csv(dataPath + \"csv_files/test/test_base.csv\")\ntest_static = pl.concat(\n    [\n        pl.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/test/test_static_0_2.csv\").pipe(set_table_dtypes),\n    ],\n    how=\"vertical_relaxed\",\n)\ntest_static_cb = pl.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\").pipe(set_table_dtypes)\ntest_person_1 = pl.read_csv(dataPath + \"csv_files/test/test_person_1.csv\").pipe(set_table_dtypes) \ntest_credit_bureau_b_2 = pl.read_csv(dataPath + \"csv_files/test/test_credit_bureau_b_2.csv\").pipe(set_table_dtypes) ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:22:52.180811Z","iopub.execute_input":"2024-05-14T04:22:52.181191Z","iopub.status.idle":"2024-05-14T04:22:52.252139Z","shell.execute_reply.started":"2024-05-14T04:22:52.181157Z","shell.execute_reply":"2024-05-14T04:22:52.250817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering\n\nIn this part, we can see a simple example of joining tables via `case_id`. Here the loading and joining is done with polars library. Polars library is blazingly fast and has much smaller memory footprint than pandas. ","metadata":{}},{"cell_type":"code","source":"# We need to use aggregation functions in tables with depth > 1, so tables that contain num_group1 column or \n# also num_group2 column.\ntrain_person_1_feats_1 = train_person_1.group_by(\"case_id\").agg(\n    pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n)\n\n# Here num_group1=0 has special meaning, it is the person who applied for the loan.\ntrain_person_1_feats_2 = train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n    pl.col(\"num_group1\") == 0\n).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n\n# Here we have num_goup1 and num_group2, so we need to aggregate again.\ntrain_credit_bureau_b_2_feats = train_credit_bureau_b_2.group_by(\"case_id\").agg(\n    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n)\n\n# We will process in this examples only A-type and M-type columns, so we need to select them.\nselected_static_cols = []\nfor col in train_static.columns:\n    if col[-1] in (\"A\", \"M\"):\n        selected_static_cols.append(col)\nprint(selected_static_cols)\n\nselected_static_cb_cols = []\nfor col in train_static_cb.columns:\n    if col[-1] in (\"A\", \"M\"):\n        selected_static_cb_cols.append(col)\nprint(selected_static_cb_cols)\n\n# Join all tables together.\ndata = train_basetable.join(\n    train_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n).join(\n    train_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n).join(\n    train_person_1_feats_1, how=\"left\", on=\"case_id\"\n).join(\n    train_person_1_feats_2, how=\"left\", on=\"case_id\"\n).join(\n    train_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n)\n\ndel train_basetable\ndel train_static_cb\ndel train_person_1_feats_2\ndel train_credit_bureau_b_2_feats\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:22:52.254172Z","iopub.execute_input":"2024-05-14T04:22:52.254657Z","iopub.status.idle":"2024-05-14T04:22:54.002488Z","shell.execute_reply.started":"2024-05-14T04:22:52.254615Z","shell.execute_reply":"2024-05-14T04:22:54.000808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n    pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n    (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n)\n\ntest_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n    pl.col(\"num_group1\") == 0\n).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n\ntest_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n    pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n    (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n)\n\ndata_submission = test_basetable.join(\n    test_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n).join(\n    test_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n).join(\n    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n).join(\n    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n).join(\n    test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:22:54.003769Z","iopub.execute_input":"2024-05-14T04:22:54.004128Z","iopub.status.idle":"2024-05-14T04:22:54.021528Z","shell.execute_reply.started":"2024-05-14T04:22:54.004098Z","shell.execute_reply":"2024-05-14T04:22:54.020355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"case_ids = data[\"case_id\"].unique().shuffle(seed=1)\ncase_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\ncase_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n\ncols_pred = []\nfor col in data.columns:\n    if col[-1].isupper() and col[:-1].islower():\n        cols_pred.append(col)\n\nprint(cols_pred)\n\ndef from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n    return (\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n    )\n\nbase_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\nbase_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\nbase_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n\nfor df in [X_train, X_valid, X_test]:\n    df = convert_strings(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:30:48.366661Z","iopub.execute_input":"2024-05-14T05:30:48.367201Z","iopub.status.idle":"2024-05-14T05:30:56.620497Z","shell.execute_reply.started":"2024-05-14T05:30:48.367159Z","shell.execute_reply":"2024-05-14T05:30:56.619084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train: {X_train.shape}\")\nprint(f\"Valid: {X_valid.shape}\")\nprint(f\"Test: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:30:56.622714Z","iopub.execute_input":"2024-05-14T05:30:56.623087Z","iopub.status.idle":"2024-05-14T05:30:56.629290Z","shell.execute_reply.started":"2024-05-14T05:30:56.623053Z","shell.execute_reply":"2024-05-14T05:30:56.628118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = X_train.loc[:, X_train.iloc[0].map(type) == str].columns.unique()\ncols","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:30:56.630811Z","iopub.execute_input":"2024-05-14T05:30:56.631149Z","iopub.status.idle":"2024-05-14T05:30:56.651259Z","shell.execute_reply.started":"2024-05-14T05:30:56.631120Z","shell.execute_reply":"2024-05-14T05:30:56.649629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.fillna(X_train.mode().iloc[0])\nX_valid = X_valid.fillna(X_train.mode().iloc[0])\nX_test = X_test.fillna(X_train.mode().iloc[0])\n\nC_X_train = X_train.copy()\nC_X_valid = X_valid.copy()\nC_X_test = X_test.copy()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:30:56.654173Z","iopub.execute_input":"2024-05-14T05:30:56.655365Z","iopub.status.idle":"2024-05-14T05:31:02.361465Z","shell.execute_reply.started":"2024-05-14T05:30:56.655302Z","shell.execute_reply":"2024-05-14T05:31:02.360224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols:\n    cat_list = {*X_train[col], *X_valid[col], *X_test[col]}\n    X_train[col] = X_train[col].astype(pd.CategoricalDtype(cat_list))\n    X_valid[col] = X_valid[col].astype(pd.CategoricalDtype(cat_list))\n    X_test[col] = X_test[col].astype(pd.CategoricalDtype(cat_list))\n\nX_train = pd.get_dummies(X_train,drop_first=True)\nX_valid = pd.get_dummies(X_valid,drop_first=True)\nX_test = pd.get_dummies(X_test,drop_first=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:31:03.319984Z","iopub.execute_input":"2024-05-14T05:31:03.320381Z","iopub.status.idle":"2024-05-14T05:31:17.725489Z","shell.execute_reply.started":"2024-05-14T05:31:03.320350Z","shell.execute_reply":"2024-05-14T05:31:17.723373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train: {X_train.shape}\")\nprint(f\"Valid: {X_valid.shape}\")\nprint(f\"Test: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:32:06.206086Z","iopub.execute_input":"2024-05-14T05:32:06.206633Z","iopub.status.idle":"2024-05-14T05:32:06.213718Z","shell.execute_reply.started":"2024-05-14T05:32:06.206590Z","shell.execute_reply":"2024-05-14T05:32:06.212535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:32:11.734520Z","iopub.execute_input":"2024-05-14T05:32:11.735030Z","iopub.status.idle":"2024-05-14T05:32:28.142358Z","shell.execute_reply.started":"2024-05-14T05:32:11.734984Z","shell.execute_reply":"2024-05-14T05:32:28.141284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train=X_train.drop_duplicates()\n# idx = (set(X_train.index) ^ set(y_train.index))\n# y_train = y_train.drop(index = idx)\n# X_valid=X_valid.drop_duplicates()\n# idx = (set(X_valid.index) ^ set(y_valid.index))\n# y_valid = y_valid.drop(index = idx)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:56:23.558548Z","iopub.execute_input":"2024-05-14T05:56:23.559055Z","iopub.status.idle":"2024-05-14T05:56:43.046971Z","shell.execute_reply.started":"2024-05-14T05:56:23.559020Z","shell.execute_reply":"2024-05-14T05:56:43.045562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = X_train.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:38:45.941604Z","iopub.execute_input":"2024-05-14T05:38:45.942109Z","iopub.status.idle":"2024-05-14T05:38:45.947989Z","shell.execute_reply.started":"2024-05-14T05:38:45.942071Z","shell.execute_reply":"2024-05-14T05:38:45.946498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 500\npca = PCA(n_components = n)\ncolumns = [f'feature_{i+1}' for i in range(n)]\nX_train = pd.DataFrame(pca.fit_transform(X_train[features]), columns=columns)\nX_valid = pd.DataFrame(pca.transform(X_valid[features]), columns=columns)\nX_test = pd.DataFrame(pca.transform(X_test[features]), columns=columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T05:38:47.734026Z","iopub.execute_input":"2024-05-14T05:38:47.734565Z","iopub.status.idle":"2024-05-14T05:39:42.578971Z","shell.execute_reply.started":"2024-05-14T05:38:47.734517Z","shell.execute_reply":"2024-05-14T05:39:42.577248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hottie = OneHotEncoder()\n# let's define the column we want to convert\nfeatures_cat = ['Comapnay','model']\ntransformer = ColumnTransformer([('One_hottie',Hottie,features_cat)],\n                            remainder = 'passthrough')\ntransformed_X = transformer.fit_transform(X)\nTransformed_X = pd.DataFrame(transformed_X)","metadata":{}},{"cell_type":"markdown","source":"## Training LightGBM\n\nMinimal example of LightGBM training is shown below.","metadata":{}},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, label=y_train)\nlgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 5,\n    \"num_leaves\": 32,\n    \"learning_rate\": 0.04,#0.05\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"n_estimators\": 3000,\n    \"verbose\": -1,\n}\nresults = {}\ngbm = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=lgb_valid,\n#     num_boost_round = 1000,\n#     callbacks=[lgb.log_evaluation(50), lgb.early_stopping(20), lgb.record_evaluation(results)]\n    callbacks=[lgb.log_evaluation(25), lgb.early_stopping(10)]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation with AUC and then comparison with the stability metric is shown below.","metadata":{}},{"cell_type":"markdown","source":"import matplotlib.pyplot as plt\nplt.plot(results['X_train']['auc'], label='train')\nplt.plot(results['X_valid']['auc'], label='valid')\nplt.ylabel('Log loss')\nplt.xlabel('Boosting round')\nplt.title('Training performance')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:55:25.752383Z","iopub.execute_input":"2024-04-14T13:55:25.752804Z","iopub.status.idle":"2024-04-14T13:55:25.794958Z","shell.execute_reply.started":"2024-04-14T13:55:25.752770Z","shell.execute_reply":"2024-04-14T13:55:25.793338Z"}}},{"cell_type":"code","source":"for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n    base[\"score\"] = y_pred\n\nprint(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \nprint(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \nprint(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n        .sort_values(\"WEEK_NUM\")\\\n        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n    \n    x = np.arange(len(gini_in_time))\n    y = gini_in_time\n    a, b = np.polyfit(x, y, 1)\n    y_hat = a*x + b\n    residuals = y - y_hat\n    res_std = np.std(residuals)\n    avg_gini = np.mean(gini_in_time)\n    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n\nstability_score_train = gini_stability(base_train)\nstability_score_valid = gini_stability(base_valid)\nstability_score_test = gini_stability(base_test)\n\nprint(f'The stability score on the train set is: {stability_score_train}') \nprint(f'The stability score on the valid set is: {stability_score_valid}') \nprint(f'The stability score on the test set is: {stability_score_test}')  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\n\nScoring the submission dataset is below, we need to take care of new categories. Then we save the score as a last step. ","metadata":{}},{"cell_type":"code","source":"X_submission = data_submission[cols_pred].to_pandas()\nX_submission = convert_strings(X_submission)\nX_submission = X_submission.fillna(X_train.mode().iloc[0])\nfor col in cols:\n    cat_list = {*C_X_train[col], *C_X_valid[col], *C_X_test[col]}\n    X_submission[col] = X_submission[col].astype(pd.CategoricalDtype(cat_list))\n\ndel C_X_train\ndel C_X_valid\ndel C_X_test\ngc.collect()\n    \nX_submission = pd.get_dummies(X_submission)\ndiff = list(set(X_submission.columns) ^ set(X_train.columns))\nX_submission.loc[:,diff] = False\ncolumns = X_train.columns\nX_submission = X_submission.reindex(columns=columns)\n\ndel X_train\ndel X_valid\ndel X_test\ngc.collect()\n\ny_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"X_submission = data_submission[cols_pred].to_pandas()\nX_submission = convert_strings(X_submission)\ncategorical_cols = X_train.select_dtypes(include=['category']).columns\n\nfor col in categorical_cols:\n    train_categories = set(X_train[col].cat.categories)\n    submission_categories = set(X_submission[col].cat.categories)\n    new_categories = submission_categories - train_categories\n    X_submission.loc[X_submission[col].isin(new_categories), col] = \"Unknown\"\n    new_dtype = pd.CategoricalDtype(categories=train_categories, ordered=True)\n    X_train[col] = X_train[col].astype(new_dtype)\n    X_submission[col] = X_submission[col].astype(new_dtype)\n\ny_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T12:49:24.515997Z","iopub.execute_input":"2024-04-15T12:49:24.517088Z","iopub.status.idle":"2024-04-15T12:49:25.464643Z","shell.execute_reply.started":"2024-04-15T12:49:24.517045Z","shell.execute_reply":"2024-04-15T12:49:25.462361Z"}}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"case_id\": data_submission[\"case_id\"].to_numpy(),\n    \"score\": y_submission_pred\n}).set_index('case_id')\nsubmission.to_csv(\"./submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best of luck, and most importantly, enjoy the process of learning and discovery! \n\n<img src=\"https://i.imgur.com/obVWIBh.png\" alt=\"Image\" width=\"700\"/>","metadata":{}}]}